---
title: "Eindopdracht_deel_2"
author: "Mohammed Al Hor"
date: "2022-10-16"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(MASS)
library(car)
library(lmtest)
library(HH)
library(ggplot2)
```

```{r}
# setwd("~/Documents/Data-Science-Business-Analytics/Data")
setwd("~/Data-Science-Business-Analytics/Data")
college_statistics <- read.csv("college_statistics.csv", header = TRUE )
# Rownames vullen met inhoud van de eerste kolom
rownames(college_statistics) <- college_statistics[,1]
# Verwijder eerste kolom
college_statistics <- college_statistics[,-1]
```


4 (a) Voer eerst een test uit voor de hypothese dat het aantal aanmeldingen een normale verdeling volgt. Wat is je conclusie? Is deze conclusie van belang voor het verder modelleren van deze variabele?

```{r}
summary(college_statistics$Apps)
hist(college_statistics$Apps)
```

Both the histogram and qq plot indicate that this variable is not normally distributed (Positive skewness in histogram and not a straight line in the qqplot). Let put this to the test using the Shapiro-Wilk test for normality.
H0: Apps is normally distributed 
Ha: Apps is not normally distributed

```{r}
shapiro.test(college_statistics$Apps)
```

The p-value is very small (less than 0.05), which means the null hypothesis can be rejected and the data is not normally distributed. However, because we have a decent sample size (777 observations) OLS remains a statistically sound method to use.


4 (b) Deel de data eerst op willekeurige manier op in een “estimation” en “test” sample. Neem 600 universiteiten in de estimation sample. Zorg ervoor dat deze opdeling reproduceerbaar is. Hint: de R-functies set.seed en sample kunnen hiervoor gebruikt worden.

```{r}
set.seed(123)

train_ind <- sample(seq_len(nrow(college_statistics)), size=600)

college_statistics_est <- college_statistics[train_ind,]
college_statistics_test <- college_statistics[-train_ind,]
```

First, we set the seed so the resulting dataframe can be reproduced. Then, we take a random sample of 600 observations. Using indexing we make an estimation dataframe and a test dataframe.


4 (c) Maak eerst een lineair model voor het aantal aanmeldingen. Gebruik hiervoor alleen de estimation sample.

```{r}
fit1 <- lm(Apps ~ Private + Top10perc + Top25perc + F.Undergrad + P.Undergrad + Outstate + Room.Board + Books + Personal + PhD + Terminal +
           S.F.Ratio + perc.alumni + Expend + Grad.Rate , data = college_statistics_est)
summary(fit1)
```
Pretty straightforward, we use the lm() function to make a linear model. Accept and Enroll are omitted, these are obviously dependent on the amount of Apps.


4 (d) Pas backward elimination toe om het aantal variabelen terug te brengen.

We do the backwards step regression by using the stepAIC function and save the results in a list

```{r}
backresults <- stepAIC(fit1, direction = "backward")
```

Then, we record and evaluate the model selected by the backwards step regression method and assign this to fit1 using the following code:

```{r}
backmodel <- backresults$call
backmodel
# This line evaluates the 'code' of the model
backmodel <- eval(backmodel)
fit1 <- backmodel
summary(fit1)
```
The formula for this model is as follows: formula = Apps ~ Private + Top10perc + F.Undergrad + P.Undergrad + Outstate + Room.Board + Personal + Terminal + perc.alumni + Expend + Grad.Rate

4 (e) Voer diverse toetsen uit om de aannamen van het lineaire model te testen. 

Now that we have a model, we can start by testing some of the assumptions of this model. First, let's take a look at the qqplot of this model.

Test for normality(A6) using qqplot:

```{r}
qqPlot(fit1)
shapiro.test(residuals(fit1))
```

A lot of the datapoints don't fall along the reference line, which means we can assume non normality. We alo observe outliers for Texas A&M Univ. at College Station and SUNY at Albany. Furthermore, the Shpiro-Wilk test on the residuals provides us the evidence to reject normality (p-value is quite small, so we can reject the null hypothesis for normality).

Test for independence(A5) using the Durbin-Watson test.

```{r}
durbinWatsonTest(fit1)
```

No autocorrelation detected (p-value is 0.382), which makes sense when we take into account the data we have(not time series data).

Let's check for linearity(A3) using component + residual plot

```{r}
crPlots(fit1)
```


Looking at these plots, we observe some non-linearity (especially in Expend and Personal). Let's put this to the test, using the the resettest from the lmtest package.

```{r}
resettest(fit1, power=2)
```

H0: linear relation between x and y
Ha: some nonlinearity

As we can see the p-value is quite small (<0.05) which means we can reject the null hypthesis and thus linearity.


Next, let's take a look at Homoskedasticity(A4) using the Breusch-Pagan test:

```{r}
ncvTest(fit1)
```

H0: constant variances (homoskedasticity)
Ha: non-constant variances (heteroskedasticity)

We can reject H0 of homoskedasticity -> variances are not constant or heteroscedasticity is present.


We can also check for multicolinearity (A7) (No perfect linear relationship in X)

```{r}
vif(fit1)
```

None are larger than 4 (this is the rule of thumb), we can assume no multicolinearity.


Because OLS is quite sensitive to outliers, let's do a quick outlier test.

```{r}
outlierTest(fit1)
```


As we previously saw in the qqplot there are some outliers, 'SUNY at Albany', 'Texas A&M Univ. at College Station' and 'University of Virginia'.


4 (f) Maak vervolgens een model voor de logaritme van het aantal aanmeldingen (ook weer met backward elimination).

The steps for this are the same as for the previous model, thus I will not go into detail.

```{r}
fit2 <- lm(log(Apps) ~ Private + Top10perc + Top25perc + F.Undergrad + P.Undergrad + Outstate + Room.Board + Books + Personal + PhD + Terminal +
             S.F.Ratio + perc.alumni + Expend + Grad.Rate , data = college_statistics_est)
summary(fit2)

# Backwards step regression
backresults <- stepAIC(fit2, direction = "backward")

# Record the best model selected by the backwards method
# This line takes the model specification as 'code'
backmodel <- backresults$call
backmodel
# This line evaluates the 'code' of the model
backmodel <- eval(backmodel)
summary(backmodel)

fit2 <- backmodel
summary(fit2)
```
Different independent variables are selected by the backwards step regression when we use the log of Apps. 
formula = log(Apps) ~ Private + Top25perc + F.Undergrad + Outstate + Room.Board + Books + PhD + S.F.Ratio + perc.alumni + Expend + Grad.Rate, data = college_statistics_est


4 (g) Voer opnieuw de diverse toetsen uit om de aannamen van het model te testen.

Test for normality


```{r}
qqPlot(fit2)
shapiro.test(residuals(fit2))
```

The data points fall along the reference line quite well compared to the previous model. We still see some outliers for 'Christendom College' and 'Texas A&M Univ. at College Station'. A Shapiro-Wilk test for normality of the residuals provides us with evidence that the residuals are not normally distributed(null hypothesis can be rejected with p-value smaller than 0.05). 

Test for independence using the Durbin-Watson test.

```{r}
durbinWatsonTest(fit2)
```

No autocorrelation detected (p-value is 0.122), which makes sense when we take into account the data we have(not time series data).


Let's check for linearity(A3) using component + residual plot

```{r}
crPlots(fit2)
```

Looking at these plots, we observe more 'non-linearity' than in the previous mdoel. (especially in F.Undergrad, S.F.Ratio, Grad.Rate). Let's put this to the test, using the the resettest from the lmtest package.

H0: linear relation between x and y
Ha: some nonlinearity

```{r}
resettest(fit2, power=2)
```

As we can see the p-value is quite small (<0.05) which means we can reject the null hypthesis and thus linearity.


Next, let's take a look at Homoskedasticity(A4) using the Breusch-Pagan test:

```{r}
ncvTest(fit2)
```

H0: constant variances (homoskedasticity)
Ha: non-constant variances (heteroskedasticity)
We fail to reject the null-hypothesis (p-value > 0.05), and thus can assume homoskedacticity is present.

A7 Multicolinearity (No perfect linear relationship in X)
```{r}
vif(fit2)
```

None are larger than 4 (this is the rule of thumb), we can assume no multicolinearity.


Because OLS is quite sensitive to outliers, let's do a quick outlier test.

```{r}
outlierTest(fit2)
```

As we previously saw in the qqplot we have one observation that was relatively far from the reference line;'Christendom College'.


4 (h) Welk van de twee modellen heeft de voorkeur?

Looking at the previous test we performed on the two models we see that the log model has a better fit (qqplot), the variances are constant (homoskedacticity) and all the variables are significant. However, let's compare these two models using 'Goodness-of-fit Measures'. Akaike's information criterion can be calculated using the AIC function in R. The smaller the AIC the better the fit of the model. 

```{r}
AIC(fit1,fit2)
```

We see that model 2 outperforms model 1, the log-likelihood value is much lower, 1025 for model 2 and 10437 for model 1. Therefore we pick model 2.


4 (i) Probeer het gekozen model nog verder te verbeteren: denk aan het toevoegen van transformaties van verklarende variabelen.

It is apparent, from the crPlot, that the relationship between the log(Apps) and the F.Undergrad variable is not linear. Let's try to do a square root transformation on this variable and compare the models.

```{r}
fit3 <- lm(formula = log(Apps) ~ Private + Top25perc + F.Undergrad + I(sqrt(F.Undergrad)) +
             Outstate + Room.Board + Books + PhD + S.F.Ratio + perc.alumni + 
             Expend + Grad.Rate, data = college_statistics_est)
```


```{r}
summary(fit3)
crPlots(fit3)
```

The crPlots presents us with much better results, we see a more linear relationship for F.Undergrad
Let's check this using AIC

```{r}
AIC(fit2,fit3)
```

The AIC value goes from 1025 to 684, the transformation results in a much better model!

Let's build on this model and perform a quadratic transformation on the S.F.Ratio (see crPlot for non linear relationship). 

```{r}
fit4 <- lm(formula = log(Apps) ~ Private + Top25perc + F.Undergrad + I(sqrt(F.Undergrad)) +
             Outstate + Room.Board + Books + PhD + S.F.Ratio + I(S.F.Ratio^2) + perc.alumni + 
             Expend + Grad.Rate, data = college_statistics_est)
summary(fit4)
crPlots(fit4)
AIC(fit3,fit4)
```

For the sake of keeping this report brief, I'm not going to show the crPlots for each of the transformation. The crPlot of S.F.Ratio is more linear and when we look at the AIC, we can see a slight decrease. The model is improving. Let's continue:

Next, we perform a quadratic transformation on 'Grad.Rate'. 

```{r}
fit5 <- lm(formula = log(Apps) ~ Private + Top25perc + F.Undergrad + I(sqrt(F.Undergrad)) +
             Outstate + Room.Board + Books + PhD + S.F.Ratio + I(S.F.Ratio^2) + perc.alumni + 
             Expend + Grad.Rate+ I(sqrt(Grad.Rate)), data = college_statistics_est)

summary(fit5)
crPlots(fit5)
AIC(fit4,fit5)
```

Again, the model goes from an AIC of 672 to 657, a slight improvement. At this point I'm content with the model, more transformation will marginally decrease the AIC and thus I am moving on to the next question.


4 (j) Hoe interpreteer je de coefficienten in het model dat je uiteindelijk hebt gevonden? Wees hierbij heel precies. Welke factoren zijn uiteindelijk het meest van belang?

Because we did a log transformation on 'Apps' in this model, we cannot directly interpret the coefficients. What this means is; if the variable (x) increases by 1 the number of 'Apps' increases approximately by 100*coefficient %. 

The most important variables are as follows: Grad.Rate, S.F.Ratio, F.Undergrad, Outstate and Expend.



4 (k) Gebruik het uiteindelijke model om voorspellingen te maken voor de waarnemingen in de estimation en de test sample.

We use the predict function to make predictions on the test and estimation sets. Exp() is used on these predictions to get the real number of Apps, instead of the log values.
```{r}
college_statistics_test$predict <- exp(predict(fit5, newdata = college_statistics_test))
college_statistics_est$predict <- exp(predict(fit5, newdata = college_statistics_est))
```


4 (l) Vergelijk de voorspelkracht (mbv. mean squared error) van het model op de estimation sample met die op de test sample. Wat concludeer je?


First let's calculate the mean squared error of both predictions.

```{r}
test_MSE <- mean((college_statistics_test$Apps - college_statistics_test$predict)^2)
est_MSE <- mean((college_statistics_est$Apps - college_statistics_est$predict)^2)
```

The MSE for the test set is 8.509.323, for the estimation set it's 1.849.956. The model obviously has more explanatory power for the data it was built on. Outside of this sample this power decreases.


## 5. In dit onderdeel voer je een ANOVA analyse uit op de relatie tussen de student faculty ratio en het percentage studenten uit de top 25% van de high school. Volg de volgende stappen:


5 (a) Maak een factor met drie levels op basis van de variabele Top25perc. De levels zijn: laag (minder dan 20%)/midden/hoog (meer dan 40%)

The code below creates a categorical variable from conditions, in the last line this variable is converted to a factor.

```{r}
college_statistics$catTop25perc[as.numeric(college_statistics$Top25perc)<20]="laag"
college_statistics$catTop25perc[as.numeric(college_statistics$Top25perc)>=20 & as.numeric(college_statistics$Top25perc)<=40]="midden"
college_statistics$catTop25perc[as.numeric(college_statistics$Top25perc)>40]="hoog"
college_statistics$catTop25perc <- as.factor(college_statistics$catTop25perc)
```


5 (b) Voer de ANOVA analyse uit en geef je conclusie(s). Presenteer de ANOVAresultaten zowel numeriek als grafisch.

First, we do a One-way ANOVA with catTop25perc on S.F.Ratio

```{r}
aov1 <- aov(S.F.Ratio~catTop25perc, data=college_statistics)
summary(aov1)
```


H0 : The mean of the dependent variable is the same across all groups
We can reject the null hypothesis (p-value < 0.05), thus there are significant differences between groups.

Some visualizations of the results.


```{r}
TukeyHSD(aov1)
```


```{r}
plot(TukeyHSD(aov1))
```


5 (c) Onderzoek of er outliers zijn. Pas de analyse aan als dat nodig is.

```{r}
outlierTest(aov1)
```

We find an outlier for Indiana Wesleyan University. Let's drop this observation and try again.

```{r}
college_statistics_outlier <- college_statistics[rownames(college_statistics) != 'Indiana Wesleyan University',]
```

Run the model again.
```{r}
aov2 <- aov(S.F.Ratio~catTop25perc, data=college_statistics_outlier)
summary(aov2)
```

H0 : The mean of the dependent variable is the same across all groups
Again, We can reject the null hypothesis (p-value < 0.05), thus there are significant differences between groups.

Some visuals:
```{r}
TukeyHSD(aov2)
plot(TukeyHSD(aov2))
```



